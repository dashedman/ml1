# Линейный Классификатор

**Линейный классификатор** - алгоритм классификации, который строит между классами разделяющую линейную поверхность, например прямую в двумерном пространстве и плоскость в трехмерном пространстве.
Часто это может быть *кусочно-линейная* поверхность, состоящая из нескольких ограниченных линейных поверхностей.

Общая формула для двух классов выглядит так:

`a(x) = sign( sum(j,1,n){ω_j * f_j(x)} - ω_0 )`

где, `(ω_0, ..., ω_n)` - параметры нашей разделяющей n-мерной линейной поверхности; `(f_1, ..., f_n)` - функции признака для обьекта выборки `x`, область значений в множестве вещественных чисел(не обязательно).


Также, для осуществеления обучения введем понятие **отступа M** - функция которая показывает знаковое растояние обьекта до построеной алгоритмом классификации поверхности.
*Суть знакового расстояния*, если граница лежит между обьектом класса и остальной массой обьектов того-же класс, значит такой обьект имеет оттрицательное расстояние - отрицательный **отступ.**
Чем больше отступ, тем сильнее обьект находится внутри своего класса.

Формула отступа для двумерного случая:

`M(ω) = y_i * f(x_i, ω)`

где, `y_i` - i-е значение класса в множестве `Y = {-1,+1}`; `f(x_i, ω)` - знакова функция растояния для точки `x_i` до поверхности `ω`;  `ω` - вектор параметров разделяющей поверхности.


Чтоб суметь оценить качество построеного классификатора введем понятие **эмпирического риска**. Он будет показывать насколько часто и сильно ошибается наш классификатор на обучающей выборке. В итоге обучение сводится к задаче **минимизации эмпирического риска**, тоесть подбор и корректировка такого классификатора чтоб эмпирический риск был наименьшим.

Функционал эмпирического риска:

`Q(a) = sum(i,1,m){ L(a(x_i), y_i) }`

где, `L(a(x_i), y_i)` - функция потерь, дающая оценку тому насколько отклонился результат работы класификатора `a` для обьекта выборки `x_i` от истиного ответа `y_i`.

(часто в составе *функции потерь* используют *отступ*)

**Содержание:**
1. [ADALINE и Правило Хэбба](#adaline-и-правило-хэбба)


## ADALINE и Правило Хэбба
[В начало](#линейный-классификатор)

### Метод стохастического градиента

**Метод стохастического градиента** - метод обучения, градиентный, при котором мы пошагово приближаем разделяющую поверхность к оптимальной. При чем, для получения оптимального приближения на каждом шаге мы выбираем лишь один из элементов выборки, случайным образом.

Итерационная формула:

`ω := ω - η*∇Q(ω)`

где, `ω` - вектор параметров разделяющей поверхности, `η` - коэффициент темпа обучения, `∇Q` - градиент нашего функционала эмпирического риска (`∇` - оператор набла).

### ADALINE

Адалайн один из методов обучения стохастического градиента на основе *минимизации эмпирического риска*, использует в качестве *функции потерь* квадратичную: `L(x, y) = (M(ω) - y)^2`, при условии что, у нас `Y = {-1,+1}`, `M(ω) = y_i*dot(ω, x_i)`.

При такой функции потерь выходит итерационная формула: `ω := ω - η*sum(i,1,m){ 2*(dot(ω, x_i) - y_i)*x_i }`


### Правило Хэбба

чертежи

#### Сравнение



## Заглавиие
[В начало](#линейный-классификатор)
Описание

Код:
```R

```
